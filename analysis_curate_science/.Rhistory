c.cles <- vda(x = pbul.c, y = cs.c, ci = T, conf = .95)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.c, pbul.c)
### Plot
p.c <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = x_crcited^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.c^(1/3)), aes(x = cs.c^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations", title = "A") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3)
## Median average yearly citation count of original findings
### Curate science data
cs.cy.n <- length(unique(RVdata[, c("orig.DOI", "y.cit")])[[2]])
cs.cy <- unique(RVdata[, c("orig.DOI", "y.cit")])[[2]]
cs.cy.iqr <- quantile(x = cs.cy, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.cy.n <- length(pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear))
pbul.cy <- pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear)
pbul.cy.iqr <- quantile(x = pbul.cy, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.cy, y = cs.cy)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.cy, pbul.cy)
### Plot
p.cy <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = cit_p_year^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.cy^(1/3)), aes(x = cs.cy^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations per year", title = "B") +
scale_x_continuous(breaks = c(0, 2, 4, 6, 8), labels = c(0, 2, 4, 6, 8)^3)
## Median sample size of original findings
### Curate science data
cs.n.n <- length(unique(RVdata[, c("orig.DOI", "sample_size")])[[2]])
cs.n <- unique(RVdata[, c("orig.DOI", "sample_size")])[[2]]
cs.n.iqr <- quantile(x = cs.n, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.n.n <- nrow(pbul.cit.df$x_n)
pbul.n <- pbul.cit.df$x_n
pbul.n.iqr <- quantile(x = pbul.n, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.n, y = cs.n)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.n, pbul.n)
### Plot
p.n <- ggplot(data = pbul.df)  +
geom_density(aes(x = x_n^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.n^(1/3)), aes(x = cs.n^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Sample size", title = "C") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3, limits = c(0, 20))
## Median replication value of original findings
RV.orig <- RVdata$RV
RV.orig.iqr <- quantile(x = RV.orig, probs = c(.25, .5, .75))
RV.orig.cles <- CLES(x = pbul.df$RV, y = RV.orig)
### Curate science data
cs.RV.n <- length(unique(RVdata[, c("orig.DOI", "RV")])[[2]])
cs.RV <- unique(RVdata[, c("orig.DOI", "RV")])[[2]]
cs.RV.iqr <- quantile(x = cs.RV, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.RV.n <- length(pbul.cit.df$RV)
pbul.RV <- pbul.cit.df$RV
pbul.RV.iqr <- quantile(x = pbul.RV, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.RV, y = cs.RV)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.RV, pbul.RV)
### Plot
p.RV <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = RV^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.RV^(1/3)), aes(x = cs.RV^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Replication value", title = "D") +
scale_x_continuous(breaks = c(0, 0.5, 1.0, 1.5, 2.0), labels = c(0, 0.5, 1.0, 1.5, 2.0)^3)
## Combine plots into grid for manuscript
grid.arrange(p.c, p.cy, p.n, p.RV)
### Plot
p.c <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = x_crcited^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.c^(1/3)), aes(x = cs.c^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations", title = "A") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3)
p.c
grid.arrange(p.c, p.cy, p.n, p.RV)
## Load Curate Science data
curate_science <- read.csv("curate_science.csv", na.strings = "", stringsAsFactors = FALSE)  # Load curate science data
curate_science$orig.N <- as.numeric(curate_science$orig.N)  # Interpret sample size as numeric variable
cursci_orig_cr <- readRDS("curate_science_orig_study_crossref.Rds")  # Load bibliometrics
curate_science$orig.citations <- cursci_citations[[2]]
pbul.df$RV <- pbul.df$cit_p_year * (1/sqrt(pbul.df$x_n))  # RV per record
pbul.cit.df$x_crcited <- pbul_citations[[2]]
pbul.df$RV <- pbul.df$cit_p_year * (1/sqrt(pbul.df$x_n))  # RV per record
## Citation count replicated studies vs. general studies
### Curate science data
cs.c.n <- length(unique(RVdata[, c("orig.DOI", "citations")])[[2]])
cs.c <- unique(RVdata[, c("orig.DOI", "citations")])[[2]]
cs.c.iqr <- quantile(x = cs.c, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.c.n <- nrow(pbul.cit.df)
pbul.c <- pbul.cit.df$x_crcited
pbul.c.iqr <- quantile(x = pbul.c, probs = c(.25, .5, .75))
### Common language effect size
set.seed(20202010)
c.cles <- vda(x = pbul.c, y = cs.c, ci = T, conf = .95)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.c, pbul.c)
### Plot
p.c <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = x_crcited^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.c^(1/3)), aes(x = cs.c^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations", title = "A") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3)
## Median average yearly citation count of original findings
### Curate science data
cs.cy.n <- length(unique(RVdata[, c("orig.DOI", "y.cit")])[[2]])
cs.cy <- unique(RVdata[, c("orig.DOI", "y.cit")])[[2]]
cs.cy.iqr <- quantile(x = cs.cy, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.cy.n <- length(pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear))
pbul.cy <- pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear)
pbul.cy.iqr <- quantile(x = pbul.cy, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.cy, y = cs.cy)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.cy, pbul.cy)
### Plot
p.cy <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = cit_p_year^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.cy^(1/3)), aes(x = cs.cy^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations per year", title = "B") +
scale_x_continuous(breaks = c(0, 2, 4, 6, 8), labels = c(0, 2, 4, 6, 8)^3)
## Median sample size of original findings
### Curate science data
cs.n.n <- length(unique(RVdata[, c("orig.DOI", "sample_size")])[[2]])
cs.n <- unique(RVdata[, c("orig.DOI", "sample_size")])[[2]]
cs.n.iqr <- quantile(x = cs.n, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.n.n <- nrow(pbul.cit.df$x_n)
pbul.n <- pbul.cit.df$x_n
pbul.n.iqr <- quantile(x = pbul.n, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.n, y = cs.n)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.n, pbul.n)
### Plot
p.n <- ggplot(data = pbul.df)  +
geom_density(aes(x = x_n^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.n^(1/3)), aes(x = cs.n^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Sample size", title = "C") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3, limits = c(0, 20))
## Median replication value of original findings
RV.orig <- RVdata$RV
RV.orig.iqr <- quantile(x = RV.orig, probs = c(.25, .5, .75))
RV.orig.cles <- CLES(x = pbul.df$RV, y = RV.orig)
### Curate science data
cs.RV.n <- length(unique(RVdata[, c("orig.DOI", "RV")])[[2]])
cs.RV <- unique(RVdata[, c("orig.DOI", "RV")])[[2]]
cs.RV.iqr <- quantile(x = cs.RV, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.RV.n <- length(pbul.cit.df$RV)
pbul.RV <- pbul.cit.df$RV
pbul.RV.iqr <- quantile(x = pbul.RV, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.RV, y = cs.RV)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.RV, pbul.RV)
### Plot
p.RV <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = RV^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.RV^(1/3)), aes(x = cs.RV^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Replication value", title = "D") +
scale_x_continuous(breaks = c(0, 0.5, 1.0, 1.5, 2.0), labels = c(0, 0.5, 1.0, 1.5, 2.0)^3)
## Combine plots into grid for manuscript
grid.arrange(p.c, p.cy, p.n, p.RV)
cursci_citations
saveRDS(cursci_citations, "curate_science_crossref_citation_count_downloaded_2020-10-20.Rds")
saveRDS(pbul_citations, "psych_bulletin_crossref_citation_count_downloaded_2020-10-20.Rds")
cursci_citations <- readRDS("curate_science_crossref_citation_count_downloaded_2020-10-20.Rds") # Download most recent citation count data
pbul_citations <- readRDS("psych_bulletin_crossref_citation_count_downloaded_2020-10-20.Rds") # Download most recent citation count data
pbul.df$x_crcited <- pbul_citations[[2]]
pbul_citations
pbul_citations[[2]]
pbul.df$x_crcited
pbul.df$x_crcited <- pbul_citations[[2]]
length(pbul_citations[[2]])
length(pbul.df$x_crcited)
pbul.cit.df$x_crcited <- pbul_citations[[2]]
pbul.df <- readRDS("psybull_meta_data.rds")
pbul_citations <- readRDS("psych_bulletin_crossref_citation_count_downloaded_2020-10-20.Rds") # Download most recent citation count data
pbul.df$years_since_pub <- 2019 - pbul.df$x_pubyear  # Years since publication
pbul.df$cit_p_year <- pbul.df$x_crcited / pbul.df$years_since_pub  # average number of citations per year
pbul.df$RV <- pbul.df$cit_p_year * (1/sqrt(pbul.df$x_n))  # RV per record
pbul.df <- pbul.df[!is.na(pbul.df$RV), ]  # Reduce data to those records for which RV can be calculated (i.e. has info on all input parameters and sample size >4)
pbul.cit.df <- pbul.df[!duplicated(pbul.df$x_doi), c("x_doi", "x_crcited", "x_pubyear", "years_since_pub", "cit_p_year", "x_n", "RV")]  # Aggregate citation data over duplicate article references
pbul.cit.df$x_crcited <- pbul_citations[[2]]
cursci_citations
cursci_citations <- readRDS("curate_science_crossref_citation_count_downloaded_2020-10-20.Rds") # Download most recent citation count data
cursci_citations
rm(list=ls())
## Load Curate Science data
curate_science <- read.csv("curate_science.csv", na.strings = "", stringsAsFactors = FALSE)  # Load curate science data
curate_science$orig.N <- as.numeric(curate_science$orig.N)  # Interpret sample size as numeric variable
cursci_orig_cr <- readRDS("curate_science_orig_study_crossref.Rds")  # Load bibliometrics
cursci_citations <- readRDS("curate_science_crossref_citation_count_downloaded_2020-10-20.Rds") # Download most recent citation count data
curate_science$orig.citations <- cursci_citations[[2]]
pbul.df <- readRDS("psybull_meta_data.rds")
pbul_citations <- readRDS("psych_bulletin_crossref_citation_count_downloaded_2020-10-20.Rds") # Download most recent citation count data
pbul.df$years_since_pub <- 2019 - pbul.df$x_pubyear  # Years since publication
pbul.df$cit_p_year <- pbul.df$x_crcited / pbul.df$years_since_pub  # average number of citations per year
pbul.df$RV <- pbul.df$cit_p_year * (1/sqrt(pbul.df$x_n))  # RV per record
pbul.df <- pbul.df[!is.na(pbul.df$RV), ]  # Reduce data to those records for which RV can be calculated (i.e. has info on all input parameters and sample size >4)
pbul.cit.df <- pbul.df[!duplicated(pbul.df$x_doi), c("x_doi", "x_crcited", "x_pubyear", "years_since_pub", "cit_p_year", "x_n", "RV")]  # Aggregate citation data over duplicate article references
pbul.cit.df$x_crcited <- pbul_citations[[2]]
years <- unlist(lapply(cursci_orig_cr, function(x) ifelse(is.null(x$`issued`$`date-parts`[1]), NA, x$`issued`$`date-parts`[1])))
rep.years <- as.numeric(str_extract(curate_science$rep.study.number, "[[:digit:]]+"))
#citations <- unlist(lapply(cursci_orig_cr, function(x) ifelse(is.null(x$`is-referenced-by-count`[1]), NA, x$`is-referenced-by-count`[1])))
doi <- unlist(lapply(cursci_orig_cr, function(x) ifelse(is.null(x$`DOI`[1]), NA, x$`DOI`[1])))
cr.df <- data.frame(orig.publ.year = years, orig.study.article.DOI = doi)
curate_science$orig.study.article.DOI <- tolower(curate_science$orig.study.article.DOI)  # set DOI characters in curate_science to lower case to match DOI formatting in cr.df
curate_science <- merge(curate_science, cr.df, by = "orig.study.article.DOI")  # Some DOIs do not match, leading to a smaller dataset.
curate_science$rep.publ.year <- rep.years
curate_science$orig.RV <- (curate_science$orig.citations / (2019 - curate_science$orig.publ.year)) * (1/sqrt(curate_science$orig.N) )  # RV=(citation_count/years) * (1/sqrt(sample_size))
RVdata <- aggregate(curate_science$orig.N, by=list(orig.DOI=curate_science$orig.study.article.DOI,
orig.study.number=curate_science$orig.study.number,
target.effect=curate_science$target.effect,
RV=curate_science$orig.RV,
#sum.RV=curate_science$sum.RV,
citations=curate_science$orig.citations,
y.cit=(curate_science$orig.citations/(2019-curate_science$orig.publ.year))
), FUN=mean)
names(RVdata)[length(names(RVdata))] <- "sample_size"
RVdata <- RVdata[order(-RVdata$RV),]
RVdata$order <- 1:nrow(RVdata)
### Curate science data
cs.c.n <- length(unique(RVdata[, c("orig.DOI", "citations")])[[2]])
cs.c <- unique(RVdata[, c("orig.DOI", "citations")])[[2]]
cs.c.iqr <- quantile(x = cs.c, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.c.n <- nrow(pbul.cit.df)
pbul.c <- pbul.cit.df$x_crcited
pbul.c.iqr <- quantile(x = pbul.c, probs = c(.25, .5, .75))
### Common language effect size
set.seed(20202010)
c.cles <- vda(x = pbul.c, y = cs.c, ci = T, conf = .95)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.c, pbul.c)
### Plot
p.c <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = x_crcited^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.c^(1/3)), aes(x = cs.c^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations", title = "A") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3)
p.c
p.c <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = x_crcited^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.c^(1/3)), aes(x = cs.c^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations", title = "A") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 30), labels = c(0, 5, 10, 15, 30)^3)
p.c
30)^3
30^3
summary(RVdata$citations)
p.c <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = x_crcited^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.c^(1/3)), aes(x = cs.c^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations", title = "A") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3)
p.c
c.cles
### Curate science data
cs.cy.n <- length(unique(RVdata[, c("orig.DOI", "y.cit")])[[2]])
cs.cy <- unique(RVdata[, c("orig.DOI", "y.cit")])[[2]]
cs.cy.iqr <- quantile(x = cs.cy, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.cy.n <- length(pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear))
pbul.cy <- pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear)
pbul.cy.iqr <- quantile(x = pbul.cy, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.cy, y = cs.cy)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.cy, pbul.cy)
### Plot
p.cy <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = cit_p_year^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.cy^(1/3)), aes(x = cs.cy^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations per year", title = "B") +
scale_x_continuous(breaks = c(0, 2, 4, 6, 8), labels = c(0, 2, 4, 6, 8)^3)
## Median sample size of original findings
### Curate science data
cs.n.n <- length(unique(RVdata[, c("orig.DOI", "sample_size")])[[2]])
cs.n <- unique(RVdata[, c("orig.DOI", "sample_size")])[[2]]
cs.n.iqr <- quantile(x = cs.n, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.n.n <- nrow(pbul.cit.df$x_n)
pbul.n <- pbul.cit.df$x_n
pbul.n.iqr <- quantile(x = pbul.n, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.n, y = cs.n)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.n, pbul.n)
### Plot
p.n <- ggplot(data = pbul.df)  +
geom_density(aes(x = x_n^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.n^(1/3)), aes(x = cs.n^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Sample size", title = "C") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3, limits = c(0, 20))
## Median replication value of original findings
RV.orig <- RVdata$RV
RV.orig.iqr <- quantile(x = RV.orig, probs = c(.25, .5, .75))
RV.orig.cles <- CLES(x = pbul.df$RV, y = RV.orig)
### Curate science data
cs.RV.n <- length(unique(RVdata[, c("orig.DOI", "RV")])[[2]])
cs.RV <- unique(RVdata[, c("orig.DOI", "RV")])[[2]]
cs.RV.iqr <- quantile(x = cs.RV, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.RV.n <- length(pbul.cit.df$RV)
pbul.RV <- pbul.cit.df$RV
pbul.RV.iqr <- quantile(x = pbul.RV, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.RV, y = cs.RV)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.RV, pbul.RV)
### Plot
p.RV <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = RV^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.RV^(1/3)), aes(x = cs.RV^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Replication value", title = "D") +
scale_x_continuous(breaks = c(0, 0.5, 1.0, 1.5, 2.0), labels = c(0, 0.5, 1.0, 1.5, 2.0)^3)
## Combine plots into grid for manuscript
grid.arrange(p.c, p.cy, p.n, p.RV)
pbul.df$x_doi
### Update citation counts for Psych. Bulletin dataset
pbul_citations <- cr_citation_count(doi = pbul.df$x_doi[1:10])
View(pbul_citations)
### Update citation counts for Psych. Bulletin dataset
pbul_citations <- cr_citation_count(doi = pbul.df$x_doi)
c.cles
c.cles
pbul_citations [[2]]
View(pbul_citations)
pbul.df$x_crcited <- pbul_citations[[2]]
pbul.df$years_since_pub <- 2019 - pbul.df$x_pubyear  # Years since publication
pbul.df$cit_p_year <- pbul.df$x_crcited / pbul.df$years_since_pub  # average number of citations per year
pbul.df$RV <- pbul.df$cit_p_year * (1/sqrt(pbul.df$x_n))  # RV per record
pbul.df <- pbul.df[!is.na(pbul.df$RV), ]  # Reduce data to those records for which RV can be calculated (i.e. has info on all input parameters and sample size >4)
pbul.cit.df <- pbul.df[!duplicated(pbul.df$x_doi), c("x_doi", "x_crcited", "x_pubyear", "years_since_pub", "cit_p_year", "x_n", "RV")]  # Aggregate citation data over duplicate article references
years <- unlist(lapply(cursci_orig_cr, function(x) ifelse(is.null(x$`issued`$`date-parts`[1]), NA, x$`issued`$`date-parts`[1])))
rep.years <- as.numeric(str_extract(curate_science$rep.study.number, "[[:digit:]]+"))
#citations <- unlist(lapply(cursci_orig_cr, function(x) ifelse(is.null(x$`is-referenced-by-count`[1]), NA, x$`is-referenced-by-count`[1])))
doi <- unlist(lapply(cursci_orig_cr, function(x) ifelse(is.null(x$`DOI`[1]), NA, x$`DOI`[1])))
cr.df <- data.frame(orig.publ.year = years, orig.study.article.DOI = doi)
curate_science$orig.study.article.DOI <- tolower(curate_science$orig.study.article.DOI)  # set DOI characters in curate_science to lower case to match DOI formatting in cr.df
curate_science <- merge(curate_science, cr.df, by = "orig.study.article.DOI")  # Some DOIs do not match, leading to a smaller dataset.
curate_science$rep.publ.year <- rep.years
# Calculate replication value for original studies in the Curate Science dataset
curate_science$orig.RV <- (curate_science$orig.citations / (2019 - curate_science$orig.publ.year)) * (1/sqrt(curate_science$orig.N) )  # RV=(citation_count/years) * (1/sqrt(sample_size))
# Aggregate results
RVdata <- aggregate(curate_science$orig.N, by=list(orig.DOI=curate_science$orig.study.article.DOI,
orig.study.number=curate_science$orig.study.number,
target.effect=curate_science$target.effect,
RV=curate_science$orig.RV,
#sum.RV=curate_science$sum.RV,
citations=curate_science$orig.citations,
y.cit=(curate_science$orig.citations/(2019-curate_science$orig.publ.year))
), FUN=mean)
names(RVdata)[length(names(RVdata))] <- "sample_size"
RVdata <- RVdata[order(-RVdata$RV),]
RVdata$order <- 1:nrow(RVdata)
#### Analyses ####
## Citation count replicated studies vs. general studies
### Curate science data
cs.c.n <- length(unique(RVdata[, c("orig.DOI", "citations")])[[2]])
cs.c <- unique(RVdata[, c("orig.DOI", "citations")])[[2]]
cs.c.iqr <- quantile(x = cs.c, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.c.n <- nrow(pbul.cit.df)
pbul.c <- pbul.cit.df$x_crcited
pbul.c.iqr <- quantile(x = pbul.c, probs = c(.25, .5, .75))
### Common language effect size
set.seed(20202010)
c.cles <- vda(x = pbul.c, y = cs.c, ci = T, conf = .95)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.c, pbul.c)
### Plot
p.c <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = x_crcited^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.c^(1/3)), aes(x = cs.c^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations", title = "A") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3)
## Median average yearly citation count of original findings
### Curate science data
cs.cy.n <- length(unique(RVdata[, c("orig.DOI", "y.cit")])[[2]])
cs.cy <- unique(RVdata[, c("orig.DOI", "y.cit")])[[2]]
cs.cy.iqr <- quantile(x = cs.cy, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.cy.n <- length(pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear))
pbul.cy <- pbul.cit.df$x_crcited/(2019-pbul.cit.df$x_pubyear)
pbul.cy.iqr <- quantile(x = pbul.cy, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.cy, y = cs.cy)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.cy, pbul.cy)
### Plot
p.cy <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = cit_p_year^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.cy^(1/3)), aes(x = cs.cy^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Citations per year", title = "B") +
scale_x_continuous(breaks = c(0, 2, 4, 6, 8), labels = c(0, 2, 4, 6, 8)^3)
## Median sample size of original findings
### Curate science data
cs.n.n <- length(unique(RVdata[, c("orig.DOI", "sample_size")])[[2]])
cs.n <- unique(RVdata[, c("orig.DOI", "sample_size")])[[2]]
cs.n.iqr <- quantile(x = cs.n, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.n.n <- nrow(pbul.cit.df$x_n)
pbul.n <- pbul.cit.df$x_n
pbul.n.iqr <- quantile(x = pbul.n, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.n, y = cs.n)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.n, pbul.n)
### Plot
p.n <- ggplot(data = pbul.df)  +
geom_density(aes(x = x_n^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.n^(1/3)), aes(x = cs.n^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Sample size", title = "C") +
scale_x_continuous(breaks = c(0, 5, 10, 15, 20), labels = c(0, 5, 10, 15, 20)^3, limits = c(0, 20))
## Median replication value of original findings
RV.orig <- RVdata$RV
RV.orig.iqr <- quantile(x = RV.orig, probs = c(.25, .5, .75))
RV.orig.cles <- CLES(x = pbul.df$RV, y = RV.orig)
### Curate science data
cs.RV.n <- length(unique(RVdata[, c("orig.DOI", "RV")])[[2]])
cs.RV <- unique(RVdata[, c("orig.DOI", "RV")])[[2]]
cs.RV.iqr <- quantile(x = cs.RV, probs = c(.25, .5, .75))
### Psychological bulletin data
pbul.RV.n <- length(pbul.cit.df$RV)
pbul.RV <- pbul.cit.df$RV
pbul.RV.iqr <- quantile(x = pbul.RV, probs = c(.25, .5, .75))
### Common language effect size
c.cles <- CLES(x = pbul.RV, y = cs.RV)  # CLES for probability x>y
### Mann-Whitney U test
wilcox.test(cs.RV, pbul.RV)
### Plot
p.RV <- ggplot(data = pbul.cit.df)  +
geom_density(aes(x = RV^(1/3)), fill = "red", alpha = 0.5, na.rm = T, adjust = 0.5) +
geom_density(data = as.data.frame(cs.RV^(1/3)), aes(x = cs.RV^(1/3)), fill = "blue", alpha = 0.5, na.rm = T, adjust = 0.5) +
theme_classic(base_size = 16) +
labs(x = "Replication value", title = "D") +
scale_x_continuous(breaks = c(0, 0.5, 1.0, 1.5, 2.0), labels = c(0, 0.5, 1.0, 1.5, 2.0)^3)
## Combine plots into grid for manuscript
grid.arrange(p.c, p.cy, p.n, p.RV)
c.cles <- vda(x = pbul.c, y = cs.c, ci = T, conf = .95)  # CLES for probability x>y
c.cles <- vda(x = pbul.c, y = cs.c, ci = T, conf = .99)  # CLES for probability x>y
c.cles
c.cles <- vda(x = cs.c, y = pbul.c, ci = T, conf = .99)  # CLES for probability x>y
set.seed(20202010)
c.cles <- vda(x = cs.c, y = pbul.c, ci = T, conf = .99)  # CLES for probability x>y
c.cles
cy.vda <- vda(x = cs.cy, y = pbul.cy, ci = T, conf = .99)  # VDA for probability x>y
cy.vda
### Common language effect size
n.vda <- vda(x = cs.n, y = pbul.n, ci = T, conf = .99)  # VDA for probability x>y
n.vda
### Common language effect size
set.seed(20202010)
RV.vda <- vda(x = cs.RV, y = pbul.RV, ci = T, conf = .99)  # VDA for probability x>y
RV.vda
View(cursci_citations)
View(curate_science)
a <- rnorm(100)
b <- rnorm(100)
c <- a*b
cor(a, c)
cor(b, c)
cor(b*a, c)
b <- rnorm(1000)
a <- rnorm(1000)
c <- a*b
cor(a, c)
cor(b, c)
cor(a, b)
c <- a+b
cor(a, c)
cor(b, c)
cor(b, a)
c <- a+b+(a*b)
cor(a, c)
cor(b, a)
cor(b, c)
b <- rnorm(10000)
a <- rnorm(10000)
c <- a*b
cor(a, c)
cor(a, b)
cor(b, c)
c <- a*2*b
cor(b, c)
cor(a, c)
